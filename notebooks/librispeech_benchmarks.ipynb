{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe9cd4b-3d98-40fc-8335-10e09a843fc1",
   "metadata": {},
   "source": [
    "# LibriSpeech Benchmarks\n",
    "\n",
    "dependecies:\n",
    "- joeynmt\n",
    "- datasets\n",
    "- transformers\n",
    "- pytorch\n",
    "- numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327c2c5-6565-42bf-9e7f-10d988f063f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48f6c3a6-5f83-4251-bfa8-8cd77f567f44",
   "metadata": {},
   "source": [
    "check package versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af6bc571-ada0-4f37-975b-abb3627c18e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.11.0+cu115\n",
      "datasets 2.3.3.dev0\n",
      "transformers 4.20.1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "print('torch', torch.__version__)\n",
    "print('datasets', datasets.__version__)\n",
    "print('transformers', transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d669c-5ce1-4788-aec3-b13bbfcf22f3",
   "metadata": {},
   "source": [
    "## metrics: WER\n",
    "\n",
    "use joeyS2T implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc43c4a-82f5-44b4-a5f2-1f18056aefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joeynmt.metrics import wer\n",
    "from joeynmt.tokenizers import EvaluationTokenizer\n",
    "\n",
    "tok = EvaluationTokenizer(lowercase=True, tokenize=\"13a\", no_punc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba5f20-7667-4e00-85b2-bb62edf56b9b",
   "metadata": {},
   "source": [
    "## data: LibriSpeech dev/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700182d2-1daf-402a-9a76-a764dbd3f297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25db8edc2ed948bfa099839d8e95faea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train.clean.100: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 28539\n",
       "    })\n",
       "    train.clean.360: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 104014\n",
       "    })\n",
       "    train.other.500: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 148688\n",
       "    })\n",
       "    validation.clean: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2703\n",
       "    })\n",
       "    validation.other: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2864\n",
       "    })\n",
       "    test.clean: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2620\n",
       "    })\n",
       "    test.other: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2939\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "librispeech_eval = load_dataset(\"librispeech_asr\", name=\"all\")\n",
    "librispeech_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf51e6-ef46-4746-b45a-39610f88fce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55e860-fc2a-4475-bbdf-f91409e53e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90561fa1-94cc-432b-b679-c0f9d4e70f09",
   "metadata": {},
   "source": [
    "## SpeechBrain\n",
    "\n",
    "https://huggingface.co/speechbrain/asr-transformer-transformerlm-librispeaech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df167714-e142-4a7c-84cc-b8bbd38a4e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function map_to_pred at 0x151eb61a1a20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5501b1928abb4002ac0a16b07dd0ad3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.clean 2.1322745487298262\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b666f6382bb940b7a587426274b72d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/716 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.other 5.513464709115176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618a1dc1be9a4fd2ad031b8f5e9fe79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.clean 2.3128423615337796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ae286c181d4004af2280f334e68226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "\n",
    "asr_model = EncoderDecoderASR.from_hparams(\n",
    "    source=\"speechbrain/asr-transformer-transformerlm-librispeech\",\n",
    "    run_opts={\"device\":\"cuda\"},\n",
    ")\n",
    "asr_model.eval()\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "def map_to_pred(batch):\n",
    "    lengths = [len(b['array']) for b in batch['audio']]\n",
    "    curr_batch_size = len(lengths)\n",
    "    max_len = max(lengths)\n",
    "    input_array = np.zeros((batch_size, max_len))\n",
    "    length_array = np.zeros((batch_size,))\n",
    "    \n",
    "    for i, b in enumerate(batch['audio']):\n",
    "        input_array[i, :lengths[i]] = b['array']\n",
    "        length_array[i] = lengths[i] / max_len\n",
    "        \n",
    "    transcription, _ = asr_model.transcribe_batch(torch.tensor(input_array), torch.tensor(length_array))\n",
    "    \n",
    "    batch[\"transcription\"] = transcription[:curr_batch_size]\n",
    "    return batch\n",
    "\n",
    "for split in ['validation.clean', 'validation.other', 'test.clean', 'test.other']:\n",
    "    result = librispeech_eval[split].map(map_to_pred,\n",
    "                                         batched=True,\n",
    "                                         batch_size=batch_size)\n",
    "                                         #remove_columns=[\"audio\"])\n",
    "    \n",
    "    print(split, wer(hypotheses=result[\"transcription\"], references=result[\"text\"], tokenizer=tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67cad03d-12ba-42f7-b154-c52c46167352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.other 5.614886422253215\n"
     ]
    }
   ],
   "source": [
    "print(split, wer(hypotheses=result[\"transcription\"], references=result[\"text\"], tokenizer=tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb03df7-b9f6-45fe-87b8-3a55c97c1abf",
   "metadata": {},
   "source": [
    "number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bef0fd8-bcb9-4a84-a07c-fd19f8e73c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164859096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#model_parameters = filter(lambda p: p.requires_grad, asr_model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in asr_model.parameters()])\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af53f7-58d1-4d35-8e4f-a78bfafa21e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f245f9db-46e2-414e-93ee-9ba4f9ada0b9",
   "metadata": {},
   "source": [
    "## facebook wav2vec2\n",
    "\n",
    "https://huggingface.co/facebook/wav2vec2-base-960h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8c53e9-0944-4cfc-85bc-73d6eb7732a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9562380a6b2442ce967e146e7c2b2423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2703 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.clean 3.167162971949561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880b1a0b992c4fbf8a793c4c2862af25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2864 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.other 8.860014132056214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89944f80e1241a79cba378c39d9f66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2620 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.clean 3.3855751673767496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178678c5a5af495b8954742bd840d7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2939 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.other 8.568480981220029\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(\"cuda\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.eval()\n",
    "\n",
    "def map_to_pred(batch):\n",
    "    input_values = processor(batch[\"audio\"][\"array\"],\n",
    "                             sampling_rate=batch[\"audio\"][\"sampling_rate\"],\n",
    "                             return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values.to(\"cuda\")).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    batch[\"transcription\"] = transcription\n",
    "    return batch\n",
    "\n",
    "for split in ['validation.clean', 'validation.other', 'test.clean', 'test.other']:\n",
    "    result = librispeech_eval[split].map(map_to_pred,\n",
    "                                         batched=False,\n",
    "                                         batch_size=1)\n",
    "                                         #remove_columns=[\"audio\"])\n",
    "\n",
    "    hyp = [s[0] for s in result[\"transcription\"]]\n",
    "    print(split, wer(hypotheses=hyp, references=result['text'], tokenizer=tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dc775-ff28-4a89-8b47-22b368fdf65b",
   "metadata": {},
   "source": [
    "number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92058553-a246-44ce-8d09-c675150b9f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94396320"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fab79f-d8d0-4a93-ad02-6ba165f01048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d68d0b-6acd-4e7d-98b2-2992b7479f97",
   "metadata": {},
   "source": [
    "## facebook s2t\n",
    "\n",
    "https://huggingface.co/facebook/s2t-medium-librispeech-asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27df3b34-36f9-43f2-97c0-a6191466012d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44aa1146e4d491e81804eb10bb976d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2703 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.clean 3.23149884195434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78909599b7cd41ab9c9f62db6fa880b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2864 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.other 8.008165188034859\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d389915130ed402eb1355586f5e9da26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2620 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.clean 3.5225197808886186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7ede9538f543628b211191f75fc58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2939 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.other 7.832948054181074\n"
     ]
    }
   ],
   "source": [
    "from transformers import Speech2TextForConditionalGeneration, Speech2TextProcessor\n",
    "import torch\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-medium-librispeech-asr\").to(\"cuda\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-medium-librispeech-asr\", do_upper_case=False)\n",
    "model.eval()\n",
    "\n",
    "def map_to_pred(batch):\n",
    "    features = processor(batch[\"audio\"][\"array\"],\n",
    "                         sampling_rate=batch[\"audio\"][\"sampling_rate\"],\n",
    "                         padding=True, return_tensors=\"pt\")\n",
    "    input_features = features.input_features.to(\"cuda\")\n",
    "    attention_mask = features.attention_mask.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model.generate(input_features=input_features, attention_mask=attention_mask)\n",
    "    \n",
    "    transcription = processor.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    batch[\"transcription\"] = transcription\n",
    "    return batch\n",
    "\n",
    "for split in ['validation.clean', 'validation.other', 'test.clean', 'test.other']:\n",
    "    result = librispeech_eval[split].map(map_to_pred,\n",
    "                                         batched=False,\n",
    "                                         batch_size=1)\n",
    "                                         #remove_columns=[\"audio\"])\n",
    "    \n",
    "    ref = [s.lower() for s in result['text']]\n",
    "    hyp = [s[0] for s in result[\"transcription\"]]\n",
    "    print(split, wer(hypotheses=hyp, references=ref, tokenizer=tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea0efc2-1135-47d8-b269-1760bb842929",
   "metadata": {},
   "source": [
    "number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f9776f-d68d-4d3f-98fb-980d667d4ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71207936"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571b4b8-211a-4b4f-ad4b-2a0f04b37a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc1f0b46-c53d-43fd-96d6-e4934de56f44",
   "metadata": {},
   "source": [
    "## joeynmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123a58c5-310d-472b-8f1f-5b0130a61097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path('/workspace/mitarb/ohta/models')\n",
    "random_seeds = [321, 42, 987]\n",
    "models = ['librispeech100h', 'librispeech960h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69c517e9-d243-48a2-8acf-6aaa1f62e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-5adcea4a61262a78.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- librispeech100h ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-7b98d6c54890e5b7.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-27804ea70aad4fbd.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-2f192d8dc95cb9bc.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-5adcea4a61262a78.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-7b98d6c54890e5b7.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-27804ea70aad4fbd.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-2f192d8dc95cb9bc.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-5adcea4a61262a78.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-7b98d6c54890e5b7.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-27804ea70aad4fbd.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-2f192d8dc95cb9bc.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-5adcea4a61262a78.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-7b98d6c54890e5b7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " clean.dev: mean=10.66 std=0.36 [11.036358957391272, 10.769824638800044, 10.179772802470497]\n",
      " other.dev: mean=23.82 std=0.34 [24.228625264976053, 23.847844861427337, 23.386590248881213]\n",
      "clean.test: mean=12.02 std=0.32 [12.334525258673159, 12.150030432136335, 11.577525867315885]\n",
      "other.test: mean=24.75 std=0.37 [25.204898458246568, 24.744473950671534, 24.310796094988824]\n",
      "\n",
      "--- librispeech960h ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-27804ea70aad4fbd.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-2f192d8dc95cb9bc.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-5adcea4a61262a78.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-7b98d6c54890e5b7.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-27804ea70aad4fbd.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-2f192d8dc95cb9bc.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-5adcea4a61262a78.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-7b98d6c54890e5b7.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-27804ea70aad4fbd.arrow\n",
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-2f192d8dc95cb9bc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " clean.dev: mean=3.79 std=0.27 [4.145068196022205, 3.501709495974413, 3.720451453990662]\n",
      " other.dev: mean=8.84 std=0.39 [9.362487241893696, 8.436052445630839, 8.728507497840935]\n",
      "clean.test: mean=4.31 std=0.52 [5.025106512477176, 3.78119293974437, 4.123554473524042]\n",
      "other.test: mean=8.66 std=0.35 [9.132071146094034, 8.318208738513269, 8.524540053111208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for model in models:\n",
    "    print('---', model, '---')\n",
    "    scores = defaultdict(list)\n",
    "    for seed in random_seeds:\n",
    "        model_dir = root_dir / f'{model}_seed{seed}'\n",
    "        for split, key in [('clean.dev', 'validation.clean'),\n",
    "                          ('other.dev', 'validation.other'),\n",
    "                          ('clean.test', 'test.clean'),\n",
    "                          ('other.test', 'test.other')]:\n",
    "            data = librispeech_eval[key].sort('id')\n",
    "            hyp = (model_dir / f'avg10_{split}').read_text().splitlines()\n",
    "            ref = [s.lower() for s in data['text']]\n",
    "            assert len(hyp) == len(ref)\n",
    "            score = wer(hypotheses=hyp, references=ref, tokenizer=tok)\n",
    "            scores[split].append(score)\n",
    "    for k, v in scores.items():\n",
    "        print('%10s: mean=%.2f std=%.2f %r' % (k, np.mean(v), np.std(v), v))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04340870-c6dc-41a1-ab40-e6f41c896aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b86ef2-2014-4f49-9855-ddfe7f6875f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7545e-d3e3-467d-85c6-4b6cf327f327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5982cdd-6be4-489f-af81-4806ec05fdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c2aee-f28d-4125-9de5-74fef0c5dafe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
